# =============================================================================
# Matric Memory - Environment Variables
# =============================================================================
# Copy this file to .env and configure for your environment.
# Variables with values shown are defaults; commented variables are optional.
#
# For Docker bundle deployment, only the variables under "Docker Bundle" are
# needed. The compose file sets sensible defaults for everything else.
#
# For standalone (non-Docker) deployment, configure all sections as needed.

# =============================================================================
# Docker Bundle Deployment
# =============================================================================
# Image tag to pull (default: bundle-latest)
# FORTEMI_TAG=bundle-latest

# External URL (REQUIRED for OAuth, MCP authentication, and public-facing links)
# This URL appears in upload guidance, export links, and OAuth metadata.
# ISSUER_URL=https://memory.example.com

# Internal API URL for MCP server → API communication (default: http://localhost:3000)
# In the Docker bundle, MCP and API run in the same container, so localhost is correct.
# Only change this for split deployments where MCP and API are separate containers.
# FORTEMI_URL=http://localhost:3000

# MCP OAuth credentials for token introspection
# Auto-managed: The bundle entrypoint auto-registers an MCP OAuth client on startup
# if credentials are missing or invalid (e.g., after a clean deploy with `down -v`).
# Credentials are persisted on the pgdata volume and reused across restarts.
# You only need to set these manually for standalone (non-Docker) deployments.
# MCP_CLIENT_ID=mm_xxx
# MCP_CLIENT_SECRET=xxx

# =============================================================================
# Database
# =============================================================================
# For standalone deployment only (Docker bundle uses POSTGRES_* vars)
DATABASE_URL=postgres://matric:matric@localhost:5432/matric

# =============================================================================
# Authentication
# =============================================================================
# Set to true to require authentication on all /api/v1/* endpoints
# When false (default), all endpoints are publicly accessible
# REQUIRE_AUTH=false

# OAuth access token lifetime in seconds (default: 3600 = 1 hour)
# Shorter = more secure, longer = less re-authentication friction
# OAUTH_TOKEN_LIFETIME_SECS=3600

# MCP token lifetime in seconds (default: 14400 = 4 hours)
# MCP sessions are interactive — shorter tokens cause mid-session disconnects
# OAUTH_MCP_TOKEN_LIFETIME_SECS=14400

# =============================================================================
# API Server
# =============================================================================
HOST=0.0.0.0
PORT=3000
RUST_LOG=info

# CORS - comma-separated list of allowed origins
# ALLOWED_ORIGINS=https://memory.example.com,http://localhost:3000

# Maximum request body size in bytes (default: 2 GB, for database backups)
# MATRIC_MAX_BODY_SIZE_BYTES=2147483648

# =============================================================================
# Rate Limiting
# =============================================================================
# RATE_LIMIT_ENABLED=true
# RATE_LIMIT_REQUESTS=100
# RATE_LIMIT_PERIOD_SECS=60

# =============================================================================
# Logging
# =============================================================================
# LOG_FORMAT=json
# LOG_FILE=/var/log/matric/api.log
# LOG_ANSI=false

# =============================================================================
# Background Worker
# =============================================================================
# WORKER_ENABLED=true
# WORKER_THREADS=4
# JOB_POLL_INTERVAL=5
# JOB_MAX_CONCURRENT=4

# =============================================================================
# File Storage
# =============================================================================
# FILE_STORAGE_PATH=/var/lib/matric/files

# =============================================================================
# Real-Time Events
# =============================================================================
# MATRIC_EVENT_BUS_CAPACITY=256
# MATRIC_WEBHOOK_TIMEOUT_SECS=10

# =============================================================================
# Full-Text Search
# =============================================================================
# FTS_SCRIPT_DETECTION=true
# FTS_TRIGRAM_FALLBACK=true
# FTS_BIGRAM_CJK=true
# FTS_MULTILINGUAL_CONFIGS=true
# FTS_WEBSEARCH_TO_TSQUERY=true

# =============================================================================
# Redis Cache
# =============================================================================
# REDIS_ENABLED=true
# REDIS_URL=redis://localhost:6379
# REDIS_CACHE_TTL=300

# =============================================================================
# Backup
# =============================================================================
# BACKUP_DEST=/var/backups/matric-memory
# BACKUP_SCRIPT_PATH=/usr/local/bin/backup-matric.sh

# =============================================================================
# Inference Provider Selection
# =============================================================================
# MATRIC_INFERENCE_DEFAULT=ollama

# =============================================================================
# Ollama (local LLM)
# =============================================================================
# OLLAMA_BASE=http://localhost:11434
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_EMBED_MODEL=nomic-embed-text
# OLLAMA_GEN_MODEL=gpt-oss:20b
# OLLAMA_EMBED_DIM=768
# MATRIC_OLLAMA_URL=http://localhost:11434
# MATRIC_OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# MATRIC_OLLAMA_GENERATION_MODEL=gpt-oss:20b
# MATRIC_EMBED_TIMEOUT_SECS=30
# MATRIC_GEN_TIMEOUT_SECS=120

# =============================================================================
# OpenAI (alternative to Ollama)
# =============================================================================
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_API_KEY=sk-xxx
# OPENAI_EMBED_MODEL=text-embedding-3-small
# OPENAI_GEN_MODEL=gpt-4o-mini
# OPENAI_EMBED_DIM=1536
# OPENAI_TIMEOUT=30
# OPENAI_SKIP_TLS_VERIFY=false
# OPENAI_HTTP_REFERER=https://memory.example.com
# OPENAI_X_TITLE=Matric Memory
# MATRIC_OPENAI_URL=https://api.openai.com/v1
# MATRIC_OPENAI_API_KEY=sk-xxx
# MATRIC_OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# MATRIC_OPENAI_GENERATION_MODEL=gpt-4o-mini

# =============================================================================
# Fast Model (extraction pipeline)
# =============================================================================
# Small model for concept tagging, reference extraction, and title generation.
# Default: qwen3:8b. Set to empty to disable.
# Large documents are automatically chunked for the fast model.
# On failure, escalates to the standard model (OLLAMA_GEN_MODEL).
# MATRIC_FAST_GEN_MODEL=qwen3:8b
# MATRIC_FAST_GEN_TIMEOUT_SECS=60

# =============================================================================
# Extraction Services
# =============================================================================
# Extraction cascade: GLiNER → fast model → standard model (failover).
# GLiNER and Whisper are enabled by default in the Docker bundle.

# GLiNER NER sidecar (zero-shot named entity recognition)
# 0.5B BERT model, CPU-only, <300ms per document, 100-200x faster than LLM.
# Enabled by default in Docker bundle. Set to empty to disable.
# GLINER_BASE_URL=http://gliner:8090
# GLINER_MODEL=urchade/gliner_large-v2.1
# GLINER_THRESHOLD=0.3

# Target number of concepts per note (default: 5).
# GLiNER runs first; if it produces fewer than this, LLM supplements.
# Higher = richer taxonomy but slower (more LLM calls).
# EXTRACTION_TARGET_CONCEPTS=5

# Maximum document frequency ratio for concepts in embedding enrichment (#475).
# Concepts appearing in more than this fraction of notes are excluded as "stopwords".
# Lower = more aggressive filtering. Range: 0.01-1.0 (default: 0.8).
# EMBED_CONCEPT_MAX_DOC_FREQ=0.8

# Instruction prefix for embedding model (#472).
# nomic-embed-text supports: "clustering: ", "search_document: ", "classification: ".
# "clustering: " maximizes inter-cluster distance for graph linking.
# Set to empty string to disable prefix.
# EMBED_INSTRUCTION_PREFIX=clustering:

# Vision model for image extraction
# Requires Ollama with vision model pulled (e.g., qwen3-vl:8b)
# OLLAMA_VISION_MODEL=qwen3-vl:8b

# Whisper transcription service
# Deploy via docker-compose.whisper.yml
# WHISPER_BASE_URL=http://host.docker.internal:8000
# WHISPER_MODEL=Systran/faster-distil-whisper-large-v3

# OCR and document processing
# OCR_ENABLED=false
# LIBREOFFICE_PATH=/usr/bin/libreoffice

# =============================================================================
# Graph Linking
# =============================================================================
# HNSW-based graph linking configuration. These tune the knowledge graph
# structure and edge weight normalization (#470, #481).

# Normalization gamma exponent for edge weights (#470).
# >1.0 compresses mid-range (emphasizes extremes), <1.0 expands mid-range.
# Range: 0.1-5.0 (default: 1.0 = linear rescaling).
# GRAPH_NORMALIZATION_GAMMA=1.0

# Keep pruned candidates in HNSW neighbor selection (default: false).
# GRAPH_KEEP_PRUNED=false

# PFNET q parameter for graph sparsification (#476).
# q=2 (default) is equivalent to Relative Neighborhood Graph.
# Higher q produces sparser graphs approaching MST.
# Range: 2-10 (default: 2).
# GRAPH_PFNET_Q=2

# Louvain community resolution parameter (#473).
# Higher = more, smaller communities. Lower = fewer, larger communities.
# Range: 0.1-10.0 (default: 1.0 = standard modularity).
# GRAPH_COMMUNITY_RESOLUTION=1.0

# SNN (Shared Nearest Neighbor) pruning threshold (#474).
# Edges with SNN score below this are pruned during recompute_snn_scores.
# SNN(A,B) = |kNN(A) ∩ kNN(B)| / k. Range: 0.0-1.0 (default: 0.10).
# GRAPH_SNN_THRESHOLD=0.10

# Structural collection edge score (#480).
# Controls the "gravity well" strength for same-collection edges in graph exploration.
# Before normalization: 0.5 is weaker than all semantic edges (0.70-0.94).
# After normalization: 0.5 is at median of [0.0, 1.0] range.
# Range: 0.0-1.0 (default: 0.5).
# GRAPH_STRUCTURAL_SCORE=0.5

# =============================================================================
# Support Memory Archive
# =============================================================================
# The fortemi-docs archive ships with product documentation pre-indexed for
# semantic search. Set to true to disable automatic loading on first boot.
# DISABLE_SUPPORT_MEMORY=false

# =============================================================================
# Multi-Memory Architecture
# =============================================================================
# Maximum concurrent live memories (default: 10). Scale with hardware.
# MAX_MEMORIES=10

# =============================================================================
# MCP Server (standalone deployment)
# =============================================================================
# MCP_TRANSPORT=http
# MCP_PORT=3001
# MCP_BASE_URL=http://localhost:3000/mcp
# MCP_BASE_PATH=/mcp
# FORTEMI_URL=http://localhost:3000
# FORTEMI_API_KEY=
