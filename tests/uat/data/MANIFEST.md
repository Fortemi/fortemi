# Test Data Manifest

Detailed specifications for each test file in the UAT data package.

## Images

### jpeg-with-exif.jpg

**Category**: Image with full EXIF metadata

**Specifications**:
- Format: JPEG
- Size: ~300-500KB
- Dimensions: 4032 x 3024 pixels (12MP, typical smartphone)
- EXIF data:
  - **GPS**: 48.8584Â°N, 2.2945Â°E (Eiffel Tower, Paris)
  - **Altitude**: 35 meters above sea level
  - **DateTime**: 2024-06-15 14:30:00 UTC
  - **Make**: Apple
  - **Model**: iPhone 15 Pro
  - **Orientation**: 1 (normal)
  - **Software**: iOS 17.5

**Content**: Landscape photo with recognizable landmarks

**Expected Extraction**:
```json
{
  "exif": {
    "datetime": "2024-06-15T14:30:00Z",
    "gps": {
      "latitude": 48.8584,
      "longitude": 2.2945,
      "altitude": 35.0
    },
    "device": {
      "make": "Apple",
      "model": "iPhone 15 Pro",
      "software": "iOS 17.5"
    },
    "dimensions": [4032, 3024],
    "orientation": 1
  },
  "provenance": {
    "location_geography": "ST_SetSRID(ST_MakePoint(2.2945, 48.8584), 4326)::geography",
    "created_at_utc": "2024-06-15T14:30:00Z"
  }
}
```

**Test Scenarios**:
- GPS coordinate extraction and PostGIS conversion
- Datetime parsing with timezone handling
- Camera device metadata extraction
- W3C PROV spatial provenance tracking

**Generation**:
```bash
python3 scripts/create-exif-images.py --preset paris-eiffel
```

---

### jpeg-no-metadata.jpg

**Category**: Image without EXIF metadata

**Specifications**:
- Format: JPEG
- Size: ~200KB
- Dimensions: 1920 x 1080 pixels
- EXIF data: None (stripped)

**Content**: Simple landscape or abstract pattern

**Expected Extraction**:
```json
{
  "exif": null,
  "provenance": null
}
```

**Test Scenarios**:
- Graceful handling of missing EXIF
- Image still accepted and stored
- No provenance data created

**Generation**:
```bash
python3 scripts/create-exif-images.py --strip-metadata landscape.jpg
```

---

### png-transparent.png

**Category**: PNG with transparency (no EXIF support)

**Specifications**:
- Format: PNG
- Size: ~50KB
- Dimensions: 512 x 512 pixels
- Transparency: Alpha channel present
- EXIF data: N/A (PNG doesn't support EXIF)

**Content**: Icon or logo with transparent background

**Expected Extraction**:
```json
{
  "format": "PNG",
  "dimensions": [512, 512],
  "has_transparency": true
}
```

**Test Scenarios**:
- PNG format support
- Transparency preservation
- No EXIF attempted (format limitation)

**Generation**:
```bash
convert -size 512x512 xc:none -fill blue -draw "circle 256,256 256,128" png-transparent.png
```

---

### webp-modern.webp

**Category**: Modern WebP format

**Specifications**:
- Format: WebP
- Size: ~100KB
- Dimensions: 1920 x 1080 pixels
- Compression: Lossy, quality 85

**Content**: General photo or scene

**Expected Extraction**:
```json
{
  "format": "WebP",
  "dimensions": [1920, 1080]
}
```

**Test Scenarios**:
- WebP format support
- Modern image format handling

**Generation**:
```bash
convert sample.jpg -quality 85 webp-modern.webp
```

---

### faces-group-photo.jpg

**Category**: Image with human faces for vision extraction

**Specifications**:
- Format: JPEG
- Size: ~400KB
- Dimensions: 2048 x 1536 pixels
- Content: Group photo with 3-5 people
- Faces: Clearly visible, various poses

**Expected Vision Extraction**:
```json
{
  "vision_description": "A group photo of five people standing outdoors in front of a building. Three women and two men, smiling and posing together. Background shows trees and a modern glass facade.",
  "detected_objects": ["person", "person", "person", "person", "person", "building", "tree"],
  "face_count": 5
}
```

**Test Scenarios**:
- AI vision model inference
- Face detection
- Scene understanding

**Source**: Download from Unsplash or generate with Stable Diffusion
```bash
# Example with Unsplash
wget "https://source.unsplash.com/2048x1536/?group,people" -O faces-group-photo.jpg
```

---

### object-scene.jpg

**Category**: Image with recognizable objects

**Specifications**:
- Format: JPEG
- Size: ~350KB
- Dimensions: 1920 x 1080 pixels
- Content: Indoor scene with common objects (laptop, coffee cup, plant, etc.)

**Expected Vision Extraction**:
```json
{
  "vision_description": "A desk workspace with a laptop computer, coffee mug, potted plant, and notebook. Natural lighting from a window on the left.",
  "detected_objects": ["laptop", "cup", "plant", "notebook", "desk", "window"]
}
```

**Test Scenarios**:
- Object detection
- Scene understanding
- Contextual description generation

**Source**: Download from Unsplash
```bash
wget "https://source.unsplash.com/1920x1080/?workspace,desk" -O object-scene.jpg
```

---

### emoji-unicode-åå‰.jpg

**Category**: Unicode filename edge case

**Specifications**:
- Format: JPEG
- Size: ~200KB
- Dimensions: 1024 x 768 pixels
- Filename: Contains emoji (ðŸŽ¨) and Japanese characters (åå‰ = "name")

**Content**: Simple photo or pattern

**Test Scenarios**:
- Unicode filename handling
- Emoji in filenames
- Japanese character support
- No mojibake (character corruption)

**Generation**:
```bash
cp sample.jpg "emoji-unicode-åå‰.jpg"
```

---

## Documents

### pdf-single-page.pdf

**Category**: Simple PDF document

**Specifications**:
- Format: PDF 1.4
- Size: ~50KB
- Pages: 1
- Content: Plain text with heading and paragraphs
- Fonts: Embedded
- Images: None

**Text Content**:
```
Test Document: Single Page PDF

This is a test document for validating PDF text extraction in matric-memory.

It contains multiple paragraphs to ensure proper text flow extraction.

Key points:
- Simple structure
- No complex formatting
- Plain text only
```

**Expected Extraction**:
```json
{
  "document_type": "pdf",
  "chunking_strategy": "per_section",
  "page_count": 1,
  "extracted_text": "Test Document: Single Page PDF\n\nThis is a test document...",
  "chunks": [
    {
      "section": "full_document",
      "content": "...",
      "char_count": 250
    }
  ]
}
```

**Test Scenarios**:
- PDF text extraction
- Single-page handling
- Document type detection from extension

**Generation**:
```bash
# Using LibreOffice headless
echo -e "Test Document: Single Page PDF\n\nThis is a test document..." > temp.txt
libreoffice --headless --convert-to pdf temp.txt --outdir documents/
mv temp.pdf documents/pdf-single-page.pdf
```

---

### code-python.py

**Category**: Python source code

**Specifications**:
- Language: Python 3.11+
- Size: ~5KB
- Lines: ~150 lines
- Features: Functions, classes, docstrings, type hints

**Content**:
```python
"""Sample Python module for testing code chunking."""

from typing import List, Optional
import json


class DataProcessor:
    """Processes data with various transformations."""

    def __init__(self, config: dict):
        self.config = config

    def process(self, data: List[dict]) -> List[dict]:
        """Process a list of data items."""
        return [self._transform(item) for item in data]

    def _transform(self, item: dict) -> dict:
        """Transform a single item."""
        # Implementation here
        return item


def main():
    processor = DataProcessor({"mode": "strict"})
    result = processor.process([{"id": 1}, {"id": 2}])
    print(json.dumps(result, indent=2))


if __name__ == "__main__":
    main()
```

**Expected Extraction**:
```json
{
  "document_type": "python",
  "chunking_strategy": "syntactic",
  "tree_sitter_language": "python",
  "chunks": [
    {"type": "import_statement", "content": "from typing import List...", "line": 3},
    {"type": "class_definition", "name": "DataProcessor", "line": 7},
    {"type": "function_definition", "name": "main", "line": 22}
  ]
}
```

**Test Scenarios**:
- Python syntax detection
- Tree-sitter syntactic chunking
- Code structure preservation

---

### code-rust.rs

**Category**: Rust source code

**Specifications**:
- Language: Rust 2021 edition
- Size: ~4KB
- Lines: ~100 lines
- Features: Structs, impl blocks, functions, traits

**Content**:
```rust
//! Sample Rust module for testing code chunking.

use std::collections::HashMap;

/// Configuration for the processor
#[derive(Debug, Clone)]
pub struct ProcessorConfig {
    pub mode: String,
    pub threshold: f64,
}

/// Main data processor
pub struct DataProcessor {
    config: ProcessorConfig,
    cache: HashMap<String, String>,
}

impl DataProcessor {
    /// Create a new processor with given config
    pub fn new(config: ProcessorConfig) -> Self {
        Self {
            config,
            cache: HashMap::new(),
        }
    }

    /// Process input data
    pub fn process(&mut self, data: &str) -> String {
        if let Some(cached) = self.cache.get(data) {
            return cached.clone();
        }

        let result = self.transform(data);
        self.cache.insert(data.to_string(), result.clone());
        result
    }

    fn transform(&self, data: &str) -> String {
        // Implementation
        data.to_uppercase()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_processor() {
        let config = ProcessorConfig {
            mode: "strict".to_string(),
            threshold: 0.5,
        };
        let mut processor = DataProcessor::new(config);
        assert_eq!(processor.process("test"), "TEST");
    }
}
```

**Expected Extraction**:
```json
{
  "document_type": "rust",
  "chunking_strategy": "syntactic",
  "tree_sitter_language": "rust",
  "chunks": [
    {"type": "use_declaration", "line": 3},
    {"type": "struct_item", "name": "ProcessorConfig", "line": 6},
    {"type": "struct_item", "name": "DataProcessor", "line": 13},
    {"type": "impl_item", "target": "DataProcessor", "line": 19},
    {"type": "mod_item", "name": "tests", "line": 44}
  ]
}
```

**Test Scenarios**:
- Rust syntax detection
- Module-level chunking
- Trait and impl block separation

---

### code-javascript.js

**Category**: JavaScript (ES6+)

**Specifications**:
- Language: JavaScript ES2022
- Size: ~3KB
- Features: Classes, arrow functions, async/await, modules

**Content**:
```javascript
/**
 * Sample JavaScript module for testing code chunking
 * @module DataProcessor
 */

import fs from 'fs/promises';

/**
 * Data processor class
 */
export class DataProcessor {
  constructor(config) {
    this.config = config;
    this.cache = new Map();
  }

  /**
   * Process data asynchronously
   * @param {Array} data - Input data
   * @returns {Promise<Array>} Processed data
   */
  async process(data) {
    const results = await Promise.all(
      data.map(item => this.transform(item))
    );
    return results;
  }

  async transform(item) {
    const cached = this.cache.get(item.id);
    if (cached) return cached;

    const result = {
      ...item,
      processed: true,
      timestamp: Date.now()
    };

    this.cache.set(item.id, result);
    return result;
  }
}

/**
 * Utility function
 */
export const loadConfig = async (path) => {
  const content = await fs.readFile(path, 'utf-8');
  return JSON.parse(content);
};

// Default export
export default DataProcessor;
```

**Expected Extraction**:
```json
{
  "document_type": "javascript",
  "chunking_strategy": "syntactic",
  "tree_sitter_language": "javascript",
  "chunks": [
    {"type": "import_statement", "line": 6},
    {"type": "class_declaration", "name": "DataProcessor", "line": 11},
    {"type": "function_declaration", "name": "loadConfig", "line": 46},
    {"type": "export_statement", "line": 52}
  ]
}
```

---

### code-typescript.ts

**Category**: TypeScript

**Specifications**:
- Language: TypeScript 5.0+
- Size: ~4KB
- Features: Interfaces, generics, type annotations, decorators

**Content**:
```typescript
/**
 * Sample TypeScript module for testing code chunking
 */

interface ProcessorConfig {
  mode: 'strict' | 'lenient';
  threshold: number;
}

interface DataItem {
  id: string;
  value: unknown;
  timestamp?: Date;
}

/**
 * Generic data processor
 */
export class DataProcessor<T extends DataItem> {
  private config: ProcessorConfig;
  private cache: Map<string, T>;

  constructor(config: ProcessorConfig) {
    this.config = config;
    this.cache = new Map();
  }

  /**
   * Process items with type safety
   */
  async process(items: T[]): Promise<T[]> {
    return Promise.all(items.map(item => this.transform(item)));
  }

  private async transform(item: T): Promise<T> {
    const cached = this.cache.get(item.id);
    if (cached) return cached;

    const result = {
      ...item,
      timestamp: new Date()
    } as T;

    this.cache.set(item.id, result);
    return result;
  }

  clearCache(): void {
    this.cache.clear();
  }
}

/**
 * Type-safe config loader
 */
export async function loadConfig(path: string): Promise<ProcessorConfig> {
  const fs = await import('fs/promises');
  const content = await fs.readFile(path, 'utf-8');
  return JSON.parse(content) as ProcessorConfig;
}
```

**Expected Extraction**:
```json
{
  "document_type": "typescript",
  "chunking_strategy": "syntactic",
  "tree_sitter_language": "typescript",
  "chunks": [
    {"type": "interface_declaration", "name": "ProcessorConfig", "line": 5},
    {"type": "interface_declaration", "name": "DataItem", "line": 10},
    {"type": "class_declaration", "name": "DataProcessor", "line": 19},
    {"type": "function_declaration", "name": "loadConfig", "line": 55}
  ]
}
```

---

### markdown-formatted.md

**Category**: Markdown with various formatting

**Specifications**:
- Format: Markdown (CommonMark)
- Size: ~8KB
- Features: Headers, lists, code blocks, tables, links, images

**Content**:
```markdown
# Test Document: Markdown Formatting

This document tests markdown chunking with semantic paragraph splitting.

## Introduction

Markdown is a lightweight markup language for creating formatted text. It supports:

- **Bold** and *italic* text
- `Inline code` snippets
- [Links](https://example.com)
- ![Images](image.jpg)

## Code Examples

Here's a Python code block:

\`\`\`python
def hello_world():
    print("Hello, World!")
\`\`\`

And a JavaScript example:

\`\`\`javascript
const greet = () => console.log("Hello!");
\`\`\`

## Tables

| Column 1 | Column 2 | Column 3 |
|----------|----------|----------|
| Data 1   | Data 2   | Data 3   |
| Data 4   | Data 5   | Data 6   |

## Lists

1. First item
2. Second item
   - Nested item A
   - Nested item B
3. Third item

## Conclusion

This document covers various markdown elements for comprehensive testing.
```

**Expected Extraction**:
```json
{
  "document_type": "markdown",
  "chunking_strategy": "semantic",
  "chunks": [
    {"section": "heading_1", "content": "Test Document: Markdown Formatting", "level": 1},
    {"section": "paragraph", "content": "This document tests...", "parent": "Introduction"},
    {"section": "heading_2", "content": "Introduction", "level": 2},
    {"section": "list", "content": "- **Bold** and *italic*...", "parent": "Introduction"},
    {"section": "heading_2", "content": "Code Examples", "level": 2},
    {"section": "code_block", "language": "python", "parent": "Code Examples"},
    {"section": "code_block", "language": "javascript", "parent": "Code Examples"},
    {"section": "heading_2", "content": "Tables", "level": 2},
    {"section": "table", "rows": 3, "cols": 3, "parent": "Tables"}
  ]
}
```

**Test Scenarios**:
- Markdown parsing
- Semantic chunking by paragraphs
- Code block extraction
- Table handling

---

### json-config.json

**Category**: JSON configuration file

**Specifications**:
- Format: JSON
- Size: ~2KB
- Structure: Nested objects and arrays
- Valid: Yes (no syntax errors)

**Content**:
```json
{
  "name": "matric-memory-test-config",
  "version": "1.0.0",
  "database": {
    "host": "localhost",
    "port": 5432,
    "name": "matric_test",
    "pool": {
      "min_connections": 2,
      "max_connections": 10,
      "timeout_seconds": 30
    }
  },
  "embedding": {
    "model": "nomic-embed-text-v1.5",
    "dimensions": 768,
    "truncate_dim": 256,
    "batch_size": 100
  },
  "search": {
    "fts_script_detection": true,
    "fts_trigram_fallback": true,
    "fts_bigram_cjk": true,
    "default_limit": 50,
    "max_limit": 500
  },
  "features": [
    "semantic_search",
    "provenance_tracking",
    "multilingual_fts",
    "document_chunking"
  ],
  "logging": {
    "level": "info",
    "format": "json",
    "outputs": ["stdout", "file"]
  }
}
```

**Expected Extraction**:
```json
{
  "document_type": "json",
  "chunking_strategy": "whole",
  "is_valid_json": true,
  "extracted_structure": {
    "top_level_keys": ["name", "version", "database", "embedding", "search", "features", "logging"],
    "depth": 3
  }
}
```

**Test Scenarios**:
- JSON parsing and validation
- Structured data extraction
- Whole-document chunking for config files

---

### yaml-config.yaml

**Category**: YAML configuration file

**Specifications**:
- Format: YAML 1.2
- Size: ~2KB
- Structure: Nested maps and sequences
- Valid: Yes

**Content**:
```yaml
name: matric-memory-test-config
version: 1.0.0

database:
  host: localhost
  port: 5432
  name: matric_test
  pool:
    min_connections: 2
    max_connections: 10
    timeout_seconds: 30

embedding:
  model: nomic-embed-text-v1.5
  dimensions: 768
  truncate_dim: 256
  batch_size: 100

search:
  fts_script_detection: true
  fts_trigram_fallback: true
  fts_bigram_cjk: true
  default_limit: 50
  max_limit: 500

features:
  - semantic_search
  - provenance_tracking
  - multilingual_fts
  - document_chunking

logging:
  level: info
  format: json
  outputs:
    - stdout
    - file
```

**Expected Extraction**:
```json
{
  "document_type": "yaml",
  "chunking_strategy": "whole",
  "is_valid_yaml": true,
  "extracted_structure": {
    "top_level_keys": ["name", "version", "database", "embedding", "search", "features", "logging"]
  }
}
```

---

### csv-data.csv

**Category**: CSV data file

**Specifications**:
- Format: CSV (RFC 4180)
- Size: ~5KB
- Rows: ~100 rows (including header)
- Columns: 5

**Content**:
```csv
id,name,email,created_at,status
1,Alice Johnson,alice@example.com,2024-01-15T10:30:00Z,active
2,Bob Smith,bob@example.com,2024-01-16T11:45:00Z,active
3,Charlie Davis,charlie@example.com,2024-01-17T09:15:00Z,inactive
4,Diana Prince,diana@example.com,2024-01-18T14:20:00Z,active
...
```

**Expected Extraction**:
```json
{
  "document_type": "csv",
  "chunking_strategy": "whole",
  "csv_metadata": {
    "columns": ["id", "name", "email", "created_at", "status"],
    "row_count": 100,
    "has_header": true
  }
}
```

**Test Scenarios**:
- CSV parsing
- Structured data handling
- Tabular data extraction

**Generation**:
```python
import csv
import random
from datetime import datetime, timedelta

with open('documents/csv-data.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(['id', 'name', 'email', 'created_at', 'status'])

    names = ['Alice Johnson', 'Bob Smith', 'Charlie Davis', 'Diana Prince', ...]
    for i in range(1, 101):
        name = random.choice(names)
        email = f"{name.split()[0].lower()}@example.com"
        created = datetime(2024, 1, 1) + timedelta(days=i)
        status = random.choice(['active', 'inactive'])
        writer.writerow([i, name, email, created.isoformat() + 'Z', status])
```

---

### config.txt

**Category**: Plain text configuration

**Specifications**:
- Format: Plain text
- Size: ~500 bytes
- Content: Simple key=value configuration

**Test Scenarios**:
- Plain text type detection
- Config file handling

---

### readme.txt

**Category**: Plain text README

**Specifications**:
- Format: Plain text
- Size: ~1KB
- Content: Project readme in plain text format

**Test Scenarios**:
- Document type detection (DOC-005, DOC-011)
- Text processing

---

### test.txt

**Category**: Plain text test document

**Specifications**:
- Format: Plain text
- Size: ~500 bytes
- Content: Simple test content

**Test Scenarios**:
- Basic text handling
- Document type detection (DOC-006)

---

### test-document.pdf

**Category**: Additional PDF test document

**Specifications**:
- Format: PDF
- Size: ~50KB
- Content: Generated test PDF for supplementary testing

**Test Scenarios**:
- PDF type detection
- Alternate PDF handling

---

## Audio

### english-speech-5s.mp3

**Category**: English speech sample

**Specifications**:
- Format: MP3
- Duration: 5 seconds
- Bitrate: 128 kbps
- Sample rate: 44.1 kHz
- Language: English (US)
- Content: Clear speech, no background noise

**Transcript**:
```
"Welcome to Matric Memory. This is a test of the audio transcription system."
```

**Expected Extraction**:
```json
{
  "document_type": "audio",
  "extraction_strategy": "whisper",
  "transcription": {
    "text": "Welcome to Matric Memory. This is a test of the audio transcription system.",
    "language": "en",
    "duration": 5.0,
    "confidence": 0.95
  }
}
```

**Test Scenarios**:
- Audio transcription with Whisper
- English language detection
- Speech-to-text accuracy

**Generation**:
```bash
# Using gTTS (Google Text-to-Speech)
python3 -c "
from gtts import gTTS
text = 'Welcome to Matric Memory. This is a test of the audio transcription system.'
tts = gTTS(text=text, lang='en', slow=False)
tts.save('english-speech-5s.mp3')
"
```

---

### spanish-greeting.mp3

**Category**: Spanish speech sample

**Specifications**:
- Format: MP3
- Duration: 3-4 seconds
- Language: Spanish (Spain)
- Content: Simple greeting

**Transcript**:
```
"Hola, bienvenido a Matric Memory."
```

**Expected Extraction**:
```json
{
  "document_type": "audio",
  "extraction_strategy": "whisper",
  "transcription": {
    "text": "Hola, bienvenido a Matric Memory.",
    "language": "es",
    "duration": 3.5
  }
}
```

**Test Scenarios**:
- Multilingual transcription
- Spanish language detection

**Generation**:
```python
from gtts import gTTS
text = "Hola, bienvenido a Matric Memory."
tts = gTTS(text=text, lang='es', slow=False)
tts.save('audio/spanish-greeting.mp3')
```

---

### chinese-phrase.mp3

**Category**: Chinese (Mandarin) speech sample

**Specifications**:
- Format: MP3
- Duration: 3-4 seconds
- Language: Chinese (Mandarin)
- Content: Simple phrase

**Transcript**:
```
"æ¬¢è¿Žä½¿ç”¨ Matric Memory"
(HuÄnyÃ­ng shÇyÃ²ng Matric Memory - Welcome to use Matric Memory)
```

**Expected Extraction**:
```json
{
  "document_type": "audio",
  "extraction_strategy": "whisper",
  "transcription": {
    "text": "æ¬¢è¿Žä½¿ç”¨ Matric Memory",
    "language": "zh",
    "duration": 3.0
  }
}
```

**Test Scenarios**:
- CJK language transcription
- Chinese character output

**Generation**:
```python
from gtts import gTTS
text = "æ¬¢è¿Žä½¿ç”¨ Matric Memory"
tts = gTTS(text=text, lang='zh-CN', slow=False)
tts.save('audio/chinese-phrase.mp3')
```

---

## Multilingual Text

### english.txt

**Content** (200-300 words):
```
The quick brown fox jumps over the lazy dog. This sentence contains every letter of the English alphabet at least once.

Natural language processing enables computers to understand, interpret, and generate human language. Modern NLP systems use transformer architectures and attention mechanisms to achieve state-of-the-art results on tasks like translation, summarization, and question answering.

Full-text search with stemming allows users to find documents even when they search for different word forms. For example, searching for "run" should also match "running", "runs", and "ran". PostgreSQL's to_tsquery function handles this automatically for English text.

Testing edge cases is crucial for robust software. Consider boundary values, empty inputs, null pointers, and unicode characters. Comprehensive test coverage catches bugs early in the development cycle.
```

**Test Query**: "running" should match "run", "runs", "ran" (stemming)

---

### german.txt

**Content** (200-300 words):
```
Die deutsche Sprache gehÃ¶rt zur westgermanischen Sprachgruppe und wird von Ã¼ber 100 Millionen Menschen gesprochen.

Volltext-Suche mit Wortstammerkennung ermÃ¶glicht es Benutzern, Dokumente zu finden, auch wenn sie nach verschiedenen Wortformen suchen. Zum Beispiel sollte die Suche nach "laufen" auch "lÃ¤uft", "lief" und "gelaufen" finden. PostgreSQL unterstÃ¼tzt deutsche Wortstammerkennung durch die entsprechende Sprachkonfiguration.

NatÃ¼rliche Sprachverarbeitung (NLP) hat in den letzten Jahren enorme Fortschritte gemacht. Moderne Systeme kÃ¶nnen Texte Ã¼bersetzen, zusammenfassen und Fragen beantworten. Die Transformer-Architektur hat dabei eine SchlÃ¼sselrolle gespielt.

Umlaute wie Ã¤, Ã¶ und Ã¼ sind wichtige Bestandteile der deutschen Schrift. Das ÃŸ (Eszett) wird in Deutschland verwendet, wÃ¤hrend in der Schweiz ss geschrieben wird.
```

**Test Query**: "laufen" should match "lÃ¤uft", "lief", "gelaufen" (German stemming)

---

### french.txt

**Content**:
```
Le franÃ§ais est une langue romane parlÃ©e par environ 300 millions de personnes dans le monde.

La recherche en texte intÃ©gral avec normalisation permet aux utilisateurs de trouver des documents mÃªme lorsqu'ils recherchent diffÃ©rentes formes de mots. Par exemple, la recherche de "courir" devrait Ã©galement correspondre Ã  "cours", "courons" et "couru". PostgreSQL prend en charge la normalisation franÃ§aise via sa configuration linguistique.

Le traitement du langage naturel (NLP) a connu des progrÃ¨s remarquables ces derniÃ¨res annÃ©es. Les systÃ¨mes modernes peuvent traduire, rÃ©sumer et rÃ©pondre aux questions. L'architecture Transformer a jouÃ© un rÃ´le clÃ© dans ces avancÃ©es.

Les accents franÃ§ais incluent l'aigu (Ã©), le grave (Ã¨), le circonflexe (Ãª) et la cÃ©dille (Ã§). Ces signes diacritiques sont essentiels pour la prononciation et le sens correct.
```

**Test Query**: "courir" should match "cours", "courons", "couru" (French stemming)

---

### spanish.txt

**Content**:
```
El espaÃ±ol es una lengua romance hablada por mÃ¡s de 500 millones de personas en todo el mundo.

La bÃºsqueda de texto completo con lematizaciÃ³n permite a los usuarios encontrar documentos incluso cuando buscan diferentes formas de palabras. Por ejemplo, buscar "correr" tambiÃ©n deberÃ­a encontrar "corre", "corriendo" y "corriÃ³". PostgreSQL admite la lematizaciÃ³n espaÃ±ola a travÃ©s de su configuraciÃ³n de idioma.

El procesamiento del lenguaje natural (PLN) ha experimentado avances notables en los Ãºltimos aÃ±os. Los sistemas modernos pueden traducir, resumir y responder preguntas. La arquitectura Transformer ha desempeÃ±ado un papel clave en estos avances.

Los acentos espaÃ±oles incluyen la tilde (Ã¡, Ã©, Ã­, Ã³, Ãº) y la diÃ©resis (Ã¼). La letra Ã± es caracterÃ­stica Ãºnica del espaÃ±ol. Los signos de interrogaciÃ³n (Â¿?) y exclamaciÃ³n (Â¡!) se usan al principio y al final de las oraciones.
```

**Test Query**: "correr" should match "corre", "corriendo", "corriÃ³" (Spanish stemming)

---

### portuguese.txt

**Content**:
```
O portuguÃªs Ã© uma lÃ­ngua romÃ¢nica falada por mais de 250 milhÃµes de pessoas em todo o mundo.

A pesquisa de texto completo com lematizaÃ§Ã£o permite que os usuÃ¡rios encontrem documentos mesmo quando pesquisam diferentes formas de palavras. Por exemplo, pesquisar "correr" tambÃ©m deve encontrar "corre", "correndo" e "correu". PostgreSQL suporta lematizaÃ§Ã£o portuguesa atravÃ©s de sua configuraÃ§Ã£o de idioma.

O processamento de linguagem natural (PLN) experimentou avanÃ§os notÃ¡veis nos Ãºltimos anos. Sistemas modernos podem traduzir, resumir e responder perguntas. A arquitetura Transformer desempenhou um papel fundamental nesses avanÃ§os.

Os acentos portugueses incluem agudo (Ã¡, Ã©), circunflexo (Ã¢, Ãª, Ã´), til (Ã£, Ãµ) e crase (Ã ). A cedilha (Ã§) tambÃ©m Ã© usada. Existem diferenÃ§as entre o portuguÃªs europeu e o brasileiro.
```

**Test Query**: "correr" should match "corre", "correndo", "correu" (Portuguese stemming)

---

### russian.txt

**Content** (Cyrillic):
```
Ð ÑƒÑÑÐºÐ¸Ð¹ ÑÐ·Ñ‹Ðº ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð²Ð¾ÑÑ‚Ð¾Ñ‡Ð½Ð¾ÑÐ»Ð°Ð²ÑÐ½ÑÐºÐ¸Ð¼ ÑÐ·Ñ‹ÐºÐ¾Ð¼ Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð±Ð¾Ð»ÐµÐµ Ñ‡ÐµÐ¼ 250 Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð°Ð¼Ð¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº Ð¿Ð¾ Ð²ÑÐµÐ¼Ñƒ Ð¼Ð¸Ñ€Ñƒ.

ÐŸÐ¾Ð»Ð½Ð¾Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ñ Ð¾ÑÐ½Ð¾Ð²Ð°Ð¼Ð¸ ÑÐ»Ð¾Ð² Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹, Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ Ð¸Ñ‰ÑƒÑ‚ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ñ‹ ÑÐ»Ð¾Ð². ÐÐ°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð¿Ð¾Ð¸ÑÐº "Ð±ÐµÐ¶Ð°Ñ‚ÑŒ" Ð´Ð¾Ð»Ð¶ÐµÐ½ Ñ‚Ð°ÐºÐ¶Ðµ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒ "Ð±ÐµÐ¶Ð¸Ñ‚", "Ð±ÐµÐ³ÑƒÑ‚" Ð¸ "Ð±ÐµÐ¶Ð°Ð»". PostgreSQL Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ñ€ÑƒÑÑÐºÐ¾Ðµ ÑÐ»Ð¾Ð²Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ ÑÐ·Ñ‹ÐºÐ¾Ð²ÑƒÑŽ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ.

ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° (NLP) Ð´Ð¾ÑÑ‚Ð¸Ð³Ð»Ð° Ð·Ð°Ð¼ÐµÑ‡Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÑƒÑÐ¿ÐµÑ…Ð¾Ð² Ð² Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ Ð³Ð¾Ð´Ñ‹. Ð¡Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð¼Ð¾Ð³ÑƒÑ‚ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ñ‚ÑŒ, Ñ€ÐµÐ·ÑŽÐ¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹. ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð° ÑÑ‹Ð³Ñ€Ð°Ð»Ð° ÐºÐ»ÑŽÑ‡ÐµÐ²ÑƒÑŽ Ñ€Ð¾Ð»ÑŒ Ð² ÑÑ‚Ð¸Ñ… Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸ÑÑ….

ÐšÐ¸Ñ€Ð¸Ð»Ð»Ð¸Ñ†Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ Ñ€ÑƒÑÑÐºÐ¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ°. Ð‘ÑƒÐºÐ²Ñ‹ Ð²ÐºÐ»ÑŽÑ‡Ð°ÑŽÑ‚ Ð°, Ð±, Ð², Ð³, Ð´, Ðµ, Ñ‘, Ð¶, Ð·, Ð¸, Ð¹, Ðº, Ð», Ð¼, Ð½, Ð¾, Ð¿, Ñ€, Ñ, Ñ‚, Ñƒ, Ñ„, Ñ…, Ñ†, Ñ‡, Ñˆ, Ñ‰, ÑŠ, Ñ‹, ÑŒ, Ñ, ÑŽ, Ñ.
```

**Test Query**: "Ð±ÐµÐ¶Ð°Ñ‚ÑŒ" should match "Ð±ÐµÐ¶Ð¸Ñ‚", "Ð±ÐµÐ³ÑƒÑ‚", "Ð±ÐµÐ¶Ð°Ð»" (Russian stemming)

---

### chinese-simplified.txt

**Content** (Simplified Chinese):
```
ä¸­æ–‡æ˜¯ä¸–ç•Œä¸Šä½¿ç”¨äººæ•°æœ€å¤šçš„è¯­è¨€ä¹‹ä¸€,æœ‰è¶…è¿‡åäº¿äººä½¿ç”¨ã€‚

å…¨æ–‡æœç´¢å¯¹äºŽä¸­æ—¥éŸ©(CJK)è¯­è¨€ä½¿ç”¨å­—ç¬¦äºŒå…ƒç»„åŒ¹é…,å› ä¸ºè¿™äº›è¯­è¨€ä¸ä½¿ç”¨ç©ºæ ¼åˆ†éš”å•è¯ã€‚PostgreSQLé€šè¿‡pg_bigmæ‰©å±•æ”¯æŒCJKæ–‡æœ¬çš„é«˜æ•ˆæœç´¢ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚çŽ°ä»£ç³»ç»Ÿå¯ä»¥ç¿»è¯‘ã€æ‘˜è¦å’Œå›žç­”é—®é¢˜ã€‚Transformeræž¶æž„åœ¨è¿™äº›è¿›å±•ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚

ä¸­æ–‡æ–‡æœ¬åŒ…å«å¸¸ç”¨æ±‰å­—ã€æ ‡ç‚¹ç¬¦å·å’Œé˜¿æ‹‰ä¼¯æ•°å­—ã€‚ç®€ä½“ä¸­æ–‡åœ¨ä¸­å›½å¤§é™†ä½¿ç”¨,è€Œç¹ä½“ä¸­æ–‡åœ¨å°æ¹¾å’Œé¦™æ¸¯ä½¿ç”¨ã€‚æœç´¢"åŒ—äº¬"åº”è¯¥èƒ½æ‰¾åˆ°åŒ…å«"åŒ—äº¬å¸‚"ã€"åŒ—äº¬å¤§å­¦"çš„æ–‡æ¡£ã€‚
```

**Test Query**: "åŒ—äº¬" should use bigram matching for "åŒ—äº¬å¸‚", "åŒ—äº¬å¤§å­¦"

---

### japanese.txt

**Content** (Japanese - Hiragana, Katakana, Kanji):
```
æ—¥æœ¬èªžã¯æ—¥æœ¬ã§è©±ã•ã‚Œã¦ã„ã‚‹è¨€èªžã§ã€ç´„1å„„2500ä¸‡äººãŒä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

å…¨æ–‡æ¤œç´¢ã¯CJKè¨€èªžã«å¯¾ã—ã¦ãƒã‚¤ã‚°ãƒ©ãƒ (2æ–‡å­—çµ„ã¿åˆã‚ã›)ãƒžãƒƒãƒãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®è¨€èªžã¯å˜èªžã‚’ç©ºç™½ã§åŒºåˆ‡ã‚‰ãªã„ãŸã‚ã€PostgreSQLã®pg_bigmæ‹¡å¼µæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦åŠ¹çŽ‡çš„ãªæ¤œç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

è‡ªç„¶è¨€èªžå‡¦ç†(NLP)æŠ€è¡“ã¯è¿‘å¹´è‘—ã—ã„é€²æ­©ã‚’é‚ã’ã¦ã„ã¾ã™ã€‚æœ€æ–°ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ç¿»è¨³ã€è¦ç´„ã€è³ªå•å¿œç­”ãŒå¯èƒ½ã§ã™ã€‚Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒã“ã‚Œã‚‰ã®é€²æ­©ã«ãŠã„ã¦é‡è¦ãªå½¹å‰²ã‚’æžœãŸã—ã¾ã—ãŸã€‚

æ—¥æœ¬èªžã®ãƒ†ã‚­ã‚¹ãƒˆã«ã¯ã€ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ãŒå«ã¾ã‚Œã¾ã™ã€‚ã€Œæ±äº¬ã€ã‚’æ¤œç´¢ã™ã‚‹ã¨ã€Œæ±äº¬éƒ½ã€ã‚„ã€Œæ±äº¬å¤§å­¦ã€ã‚’å«ã‚€æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚‹ã¯ãšã§ã™ã€‚
```

**Test Query**: "æ±äº¬" should use bigram matching for "æ±äº¬éƒ½", "æ±äº¬å¤§å­¦"

---

### korean.txt

**Content** (Korean - Hangul):
```
í•œêµ­ì–´ëŠ” í•œêµ­ê³¼ ë¶í•œì—ì„œ ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ë¡œ ì•½ 7700ë§Œ ëª…ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰ì€ CJK ì–¸ì–´ì— ëŒ€í•´ ë°”ì´ê·¸ëž¨(2ê¸€ìž ì¡°í•©) ë§¤ì¹­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì–¸ì–´ëŠ” ê³µë°±ìœ¼ë¡œ ë‹¨ì–´ë¥¼ êµ¬ë¶„í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— PostgreSQLì˜ pg_bigm í™•ìž¥ì„ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

ìžì—°ì–´ ì²˜ë¦¬(NLP) ê¸°ìˆ ì€ ìµœê·¼ ëª‡ ë…„ê°„ í˜„ì €í•œ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. ìµœì‹  ì‹œìŠ¤í…œì€ ë²ˆì—­, ìš”ì•½, ì§ˆë¬¸ ì‘ë‹µì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. Transformer ì•„í‚¤í…ì²˜ê°€ ì´ëŸ¬í•œ ë°œì „ì— í•µì‹¬ì ì¸ ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤.

í•œêµ­ì–´ í…ìŠ¤íŠ¸ëŠ” í•œê¸€ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. "ì„œìš¸"ì„ ê²€ìƒ‰í•˜ë©´ "ì„œìš¸ì‹œ"ë‚˜ "ì„œìš¸ëŒ€í•™êµ"ê°€ í¬í•¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
```

**Test Query**: "ì„œìš¸" should use bigram matching for "ì„œìš¸ì‹œ", "ì„œìš¸ëŒ€í•™êµ"

---

### arabic.txt

**Content** (Arabic - RTL):
```
Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ø¥Ø­Ø¯Ù‰ Ø£ÙƒØ«Ø± Ø§Ù„Ù„ØºØ§Øª Ø§Ù†ØªØ´Ø§Ø±Ù‹Ø§ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ Ø­ÙŠØ« ÙŠØªØ­Ø¯Ø« Ø¨Ù‡Ø§ Ø£ÙƒØ«Ø± Ù…Ù† 400 Ù…Ù„ÙŠÙˆÙ† Ø´Ø®Øµ.

ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„ØºØ§Øª Ø§Ù„ØªÙŠ ØªÙÙƒØªØ¨ Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø± Ù…Ø«Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ØµØ­ÙŠØ­. ÙŠØ¯Ø¹Ù… PostgreSQL Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨.

Ø´Ù‡Ø¯Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ØªÙ‚Ø¯Ù…Ù‹Ø§ Ù…Ù„Ø­ÙˆØ¸Ù‹Ø§ ÙÙŠ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©. ÙŠÙ…ÙƒÙ† Ù„Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©. Ù„Ø¹Ø¨Øª Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø­ÙˆÙ„ Ø¯ÙˆØ±Ù‹Ø§ Ø±Ø¦ÙŠØ³ÙŠÙ‹Ø§ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØªØ·ÙˆØ±Ø§Øª.

Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙŠØªØ¶Ù…Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù…Ø«Ù„ Ø§Ù„ÙØªØ­Ø© ÙˆØ§Ù„ÙƒØ³Ø±Ø© ÙˆØ§Ù„Ø¶Ù…Ø©. Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ØªÙÙƒØªØ¨ Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø± ÙˆØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 28 Ø­Ø±ÙÙ‹Ø§.
```

**Test Query**: Basic tokenization (no stemming in current implementation)

---

### greek.txt

**Content** (Greek):
```
Î— ÎµÎ»Î»Î·Î½Î¹ÎºÎ® Î³Î»ÏŽÏƒÏƒÎ± ÎµÎ¯Î½Î±Î¹ Î¼Î¯Î± Î±Ï€ÏŒ Ï„Î¹Ï‚ Î±ÏÏ‡Î±Î¹ÏŒÏ„ÎµÏÎµÏ‚ Î³Î»ÏŽÏƒÏƒÎµÏ‚ ÏƒÏ„Î¿Î½ ÎºÏŒÏƒÎ¼Î¿ ÎºÎ±Î¹ Î¿Î¼Î¹Î»ÎµÎ¯Ï„Î±Î¹ Î±Ï€ÏŒ Ï€ÎµÏÎ¯Ï€Î¿Ï… 13 ÎµÎºÎ±Ï„Î¿Î¼Î¼ÏÏÎ¹Î± Î±Î½Î¸ÏÏŽÏ€Î¿Ï…Ï‚.

Î— Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Ï€Î»Î®ÏÎ¿Ï…Ï‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Î³Î¹Î± Ï„Î·Î½ ÎµÎ»Î»Î·Î½Î¹ÎºÎ® Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î²Î±ÏƒÎ¹ÎºÎ® Ï„Î¼Î·Î¼Î±Ï„Î¿Ï€Î¿Î¯Î·ÏƒÎ·. Î¤Î¿ PostgreSQL Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶ÎµÎ¹ ÎµÎ»Î»Î·Î½Î¹ÎºÏŒ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Î¼Î­ÏƒÏ‰ Ï„Î·Ï‚ ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î·Ï‚ Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ®Ï‚ Î´Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ·Ï‚.

Î— ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï†Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î³Î»ÏŽÏƒÏƒÎ±Ï‚ Î­Ï‡ÎµÎ¹ ÏƒÎ·Î¼ÎµÎ¹ÏŽÏƒÎµÎ¹ Î±Î¾Î¹Î¿ÏƒÎ·Î¼ÎµÎ¯Ï‰Ï„Î· Ï€ÏÏŒÎ¿Î´Î¿ Ï„Î± Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î± Ï‡ÏÏŒÎ½Î¹Î±. Î¤Î± ÏƒÏÎ³Ï‡ÏÎ¿Î½Î± ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î± Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Î¼ÎµÏ„Î±Ï†ÏÎ¬Î¶Î¿Ï…Î½, Î½Î± ÏƒÏ…Î½Î¿ÏˆÎ¯Î¶Î¿Ï…Î½ ÎºÎ±Î¹ Î½Î± Î±Ï€Î±Î½Ï„Î¿ÏÎ½ ÏƒÎµ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚.

Î¤Î¿ ÎµÎ»Î»Î·Î½Î¹ÎºÏŒ Î±Î»Ï†Î¬Î²Î·Ï„Î¿ Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ Î³ÏÎ¬Î¼Î¼Î±Ï„Î± ÏŒÏ€Ï‰Ï‚ Î±, Î², Î³, Î´, Îµ, Î¶, Î·, Î¸, Î¹, Îº, Î», Î¼, Î½, Î¾, Î¿, Ï€, Ï, Ïƒ, Ï„, Ï…, Ï†, Ï‡, Ïˆ, Ï‰.
```

**Test Query**: Basic tokenization

---

### hebrew.txt

**Content** (Hebrew - RTL):
```
×”×¢×‘×¨×™×ª ×”×™× ×©×¤×” ×©×ž×™×ª ×”×ž×“×•×‘×¨×ª ×¢×œ ×™×“×™ ×›-9 ×ž×™×œ×™×•×Ÿ ×× ×©×™× ×‘×¨×—×‘×™ ×”×¢×•×œ×.

×—×™×¤×•×© ×˜×§×¡×˜ ×ž×œ× ×œ×©×¤×•×ª ×”× ×›×ª×‘×•×ª ×ž×™×ž×™×Ÿ ×œ×©×ž××œ ×›×ž×• ×¢×‘×¨×™×ª ×ž×©×ª×ž×© ×‘×§×™×“×•×“ × ×›×•×Ÿ. PostgreSQL ×ª×•×ž×š ×‘×˜×§×¡×˜ ×¢×‘×¨×™ ×‘××ž×¦×¢×•×ª ×ª×¦×•×¨×ª ×”×©×¤×” ×”×ž×ª××™×ž×”.

×¢×™×‘×•×“ ×©×¤×” ×˜×‘×¢×™×ª ×—×•×•×” ×”×ª×§×“×ž×•×ª × ×™×›×¨×ª ×‘×©× ×™× ×”××—×¨×•× ×•×ª. ×ž×¢×¨×›×•×ª ×ž×•×“×¨× ×™×•×ª ×™×›×•×œ×•×ª ×œ×ª×¨×’×, ×œ×¡×›× ×•×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª. ××¨×›×™×˜×§×˜×•×¨×ª ×”×˜×¨× ×¡×¤×•×¨×ž×¨ ×©×™×—×§×” ×ª×¤×§×™×“ ×ž×¨×›×–×™ ×‘×”×ª×§×“×ž×•×ª ×–×•.

×”×˜×§×¡×˜ ×”×¢×‘×¨×™ ×›×•×œ×œ × ×™×§×•×“ ××š ×‘×“×¨×š ×›×œ×œ × ×›×ª×‘ ×‘×œ×™ ××•×ª×•. ×”××œ×¤×‘×™×ª ×”×¢×‘×¨×™ ×ž×›×™×œ 22 ××•×ª×™×•×ª.
```

**Test Query**: Basic tokenization (RTL support)

---

### emoji-heavy.txt

**Content**:
```
ðŸŽ‰ Welcome to Matric Memory! ðŸš€

Full-text search supports emoji through trigram indexing. ðŸ”âœ¨

Common emoji usage:
- ðŸ˜€ðŸ˜ðŸ˜‚ðŸ¤£ Happy faces
- ðŸ”¥ðŸ’¯ðŸ‘ Positive reactions
- ðŸŒŸâ­âœ¨ Stars and sparkles
- ðŸŽ¯ðŸŽ¨ðŸŽ­ Activities
- ðŸŒðŸŒŽðŸŒ World globes
- ðŸ’»ðŸ“±âŒ¨ï¸ Technology
- ðŸ•ðŸ”ðŸŸ Food

Emoji can be searched individually: ðŸŽ‰ or combined: ðŸš€ðŸŒŸ

PostgreSQL's pg_trgm extension enables substring matching for emoji characters, allowing users to search for "ðŸŽ‰" and find all documents containing that specific emoji. ðŸŽŠðŸŽˆ
```

**Test Query**: "ðŸŽ‰" should find documents with that emoji (trigram matching)

---

## Edge Cases

### empty.txt

**Specifications**:
- Size: 0 bytes
- Content: None

**Expected Behavior**:
- HTTP 200 (accept the file)
- Warning in metadata: "empty_content": true
- No FTS indexing (nothing to index)
- Note created with empty content

**Test**:
```bash
curl -X POST http://localhost:3000/api/v1/notes \
  -F "content=@edge-cases/empty.txt" \
  -F "tags=test,edge-case"

# Response should include:
# "metadata": {"warnings": ["File is empty"]}
```

---

### large-text-100kb.txt

**Specifications**:
- Size: ~100KB (>100,000 bytes)
- Content: Repeated lorem ipsum text to reach size threshold
- Format: Plain text

**Expected Behavior**:
- Appropriate chunking based on document type (semantic for plain text)
- Multiple chunks created (likely 5-10 chunks depending on chunk size)
- All chunks indexed for FTS
- Memory-efficient processing (streaming if possible)

**Generation**:
```python
with open('edge-cases/large-text-100kb.txt', 'w') as f:
    lorem = "Lorem ipsum dolor sit amet, consectetur adipiscing elit..."
    while f.tell() < 100000:
        f.write(lorem + "\n\n")
```

---

### binary-wrong-ext.jpg

**Specifications**:
- Actual format: Random binary data (not an image)
- Extension: `.jpg` (misleading)
- Size: ~10KB
- Magic bytes: Random (not JPEG magic bytes FF D8 FF)

**Expected Behavior**:
- EXIF extraction fails with clear error
- HTTP 400 or similar error response
- Error message: "Invalid image format" or "Failed to read EXIF data"
- File rejected, note not created

**Generation**:
```python
import os
with open('edge-cases/binary-wrong-ext.jpg', 'wb') as f:
    f.write(os.urandom(10240))  # 10KB random bytes
```

**Test**:
```bash
curl -X POST http://localhost:3000/api/v1/notes \
  -F "content=@edge-cases/binary-wrong-ext.jpg" \
  -F "tags=test,edge-case"

# Expected: HTTP 400
# {"error": "Invalid image format: Failed to read EXIF data"}
```

---

### unicode-filename-æµ‹è¯•.txt

**Specifications**:
- Filename: Contains Chinese characters (æµ‹è¯• = "test")
- Content: "This file has Unicode in its filename: æµ‹è¯•"
- Size: ~100 bytes

**Expected Behavior**:
- Filename stored correctly in database
- No mojibake (ï¿½ï¿½ characters)
- File accessible via API with correct filename
- Content searchable normally

**Test**:
```bash
curl -X POST http://localhost:3000/api/v1/notes \
  -F "content=@edge-cases/unicode-filename-æµ‹è¯•.txt" \
  -F "tags=test,unicode"

# Verify filename in response
curl http://localhost:3000/api/v1/notes/{note_id} | jq '.note.metadata.filename'
# Should show: "unicode-filename-æµ‹è¯•.txt"
```

---

### whitespace-only.txt

**Specifications**:
- Content: Only whitespace (spaces, tabs, newlines)
- Size: ~500 bytes
- Example: "    \n\t\t  \n\n    \n"

**Expected Behavior**:
- Accept file (HTTP 200)
- Mark as empty content after trimming
- No FTS indexing (nothing meaningful to index)
- Metadata flag: "empty_after_trim": true

**Generation**:
```python
with open('edge-cases/whitespace-only.txt', 'w') as f:
    f.write("    \n\t\t  \n\n    \n" * 20)
```

---

### malformed-json.json

**Specifications**:
- Format: JSON (claimed)
- Content: Invalid JSON syntax
- Size: ~500 bytes

**Content**:
```json
{
  "name": "test",
  "value": 123,
  "nested": {
    "key": "value"
    "missing_comma": true
  },
  "trailing_comma": true,
}
```

**Expected Behavior**:
- JSON parsing fails
- Fallback to plain text storage
- Document type: "text" instead of "json"
- Content stored as-is (no parsing)
- Warning in metadata: "json_parse_failed": true

**Test**:
```bash
curl -X POST http://localhost:3000/api/v1/notes \
  -F "content=@edge-cases/malformed-json.json" \
  -F "tags=test,edge-case"

# Check document type
curl http://localhost:3000/api/v1/notes/{note_id} | jq '.note.document_type_name'
# Should be "text" not "json"
```

---

### malware.exe

**Category**: Suspicious file extension edge case

**Specifications**:
- Format: Executable file extension
- Size: ~100 bytes
- Content: Harmless placeholder (not actual malware)

**Test Scenarios**:
- Dangerous file extension handling
- Upload security filtering

---

### script.sh

**Category**: Script file upload edge case

**Specifications**:
- Format: Shell script
- Size: ~200 bytes
- Content: Simple bash script

**Test Scenarios**:
- Script upload handling
- Executable file type detection

---

## Provenance

### paris-eiffel-tower.jpg

**Specifications**:
- GPS: 48.8584Â°N, 2.2945Â°E (Eiffel Tower, Paris, France)
- Altitude: 35 meters
- DateTime: 2024-07-14T12:00:00Z (Bastille Day)
- Camera: Canon EOS R5
- Dimensions: 3840 x 2160

**Expected Provenance**:
```sql
SELECT
  n.id,
  n.title,
  ST_AsText(p.location_geography::geometry) as location,
  p.created_at_utc,
  p.device_info->>'make' as camera_make
FROM note n
JOIN provenance_edge p ON n.id = p.revision_id
WHERE n.title LIKE '%paris%';

-- Result:
-- location: POINT(2.2945 48.8584)
-- created_at_utc: 2024-07-14 12:00:00+00
-- camera_make: Canon
```

---

### newyork-statue-liberty.jpg

**Specifications**:
- GPS: 40.6892Â°N, 74.0445Â°W (Statue of Liberty, New York, USA)
- Altitude: 10 meters
- DateTime: 2024-07-04T16:30:00Z (Independence Day)
- Camera: Nikon Z9
- Dimensions: 4096 x 2732

**Expected Provenance**:
- Location: PostGIS geography point in New York Harbor
- Temporal: July 4, 2024

---

### tokyo-shibuya.jpg

**Specifications**:
- GPS: 35.6595Â°N, 139.7004Â°E (Shibuya Crossing, Tokyo, Japan)
- Altitude: 30 meters
- DateTime: 2024-03-21T09:00:00Z
- Camera: Sony Î±7R V
- Dimensions: 4320 x 2880

**Expected Provenance**:
- Location: PostGIS geography point in Tokyo
- Temporal: March 21, 2024

---

### dated-2020-01-01.jpg

**Specifications**:
- GPS: None
- DateTime: 2020-01-01T00:00:00Z (millennium edge case)
- Camera: iPhone 11
- Dimensions: 3024 x 4032 (portrait)

**Test Scenario**: Temporal provenance tracking for historical date

---

### dated-2025-12-31.jpg

**Specifications**:
- GPS: None
- DateTime: 2025-12-31T23:59:59Z (end of year edge case)
- Camera: Pixel 9 Pro
- Dimensions: 4080 x 3072

**Test Scenario**: Future date handling (if current date < 2025-12-31)

---

### duplicate-content-1.txt

**Content**:
```
This is duplicate content for testing content-based deduplication.

The hash of this content should match duplicate-content-2.txt exactly.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
```

**Hash (SHA-256)**: (Calculated on upload)

---

### duplicate-content-2.txt

**Content**: (Identical to duplicate-content-1.txt)
```
This is duplicate content for testing content-based deduplication.

The hash of this content should match duplicate-content-2.txt exactly.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
```

**Expected Behavior**:
```sql
SELECT hash, COUNT(*) as count, array_agg(id) as note_ids
FROM note_original
GROUP BY hash
HAVING COUNT(*) > 1;

-- Result:
-- hash: <same_hash>
-- count: 2
-- note_ids: {uuid1, uuid2}
```

**Test Scenario**: Content deduplication detection via hash matching

---

## Summary Statistics

| Category | Count | Total Size |
|----------|-------|------------|
| Images | 7 | ~2.5 MB |
| Documents | 14 | ~200 KB |
| Audio | 3 | ~300 KB |
| Multilingual | 13 | ~50 KB |
| Edge Cases | 8 | ~120 KB |
| Provenance | 7 | ~3 MB |
| **Total** | **52** | **~6.2 MB** |

All test files combined should be under 10 MB to keep the repository lean.
