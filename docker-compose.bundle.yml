# docker-compose.bundle.yml - All-in-one Fortemi deployment
#
# Usage (development/UAT - pulls from internal Gitea registry):
#   docker compose -f docker-compose.bundle.yml pull
#   docker compose -f docker-compose.bundle.yml up -d --no-build
#
# Usage (development - build locally):
#   docker compose -f docker-compose.bundle.yml up -d --build
#
# Usage (external/end-user - pull from GHCR):
#   FORTEMI_REGISTRY=ghcr.io FORTEMI_TAG=bundle-latest \
#     docker compose -f docker-compose.bundle.yml up -d
#
# Audio transcription (Whisper) is enabled by default:
#   - GPU mode (default): Uses NVIDIA GPU for fast transcription
#   - No GPU? App still starts, transcription disabled until you enable CPU mode:
#     docker compose -f docker-compose.bundle.yml --profile whisper-cpu up -d
#
# For clean install (wipe database):
#   docker compose -f docker-compose.bundle.yml down -v
#   docker compose -f docker-compose.bundle.yml up -d
#
# Environment: Create .env from .env.example, set ISSUER_URL + MCP credentials.
# After first start, register MCP client: POST /oauth/register
# Then set MCP_CLIENT_ID/MCP_CLIENT_SECRET in .env and recreate container.

services:
  # Redis cache for search query results
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - matric-redis:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Whisper speech-to-text server (GPU-accelerated, OpenAI-compatible API)
  # Enabled by default with GPU. For CPU-only: use --profile whisper-cpu instead
  whisper:
    image: ghcr.io/speaches-ai/speaches:latest-cuda-12.6.3
    volumes:
      - whisper-models:/home/ubuntu/.cache/huggingface/hub
    environment:
      - WHISPER__INFERENCE_DEVICE=auto
      - WHISPER__COMPUTE_TYPE=float16
      # Pre-download model on startup (avoids first-request delay)
      - PRELOAD_MODELS=["${WHISPER_MODEL:-Systran/faster-distil-whisper-large-v3}"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://0.0.0.0:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Whisper CPU-only variant (slower but works without NVIDIA GPU)
  # Activate with: docker compose --profile whisper-cpu up -d
  whisper-cpu:
    image: ghcr.io/speaches-ai/speaches:latest-cpu
    profiles:
      - whisper-cpu
    volumes:
      - whisper-models:/home/ubuntu/.cache/huggingface/hub
    environment:
      - WHISPER__INFERENCE_DEVICE=cpu
      - WHISPER__COMPUTE_TYPE=int8
      - PRELOAD_MODELS=["${WHISPER_MODEL:-Systran/faster-distil-whisper-large-v3}"]
    networks:
      default:
        aliases:
          - whisper  # Allow matric to reach this as http://whisper:8000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://0.0.0.0:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # GLiNER NER sidecar (zero-shot entity extraction, CPU-only)
  # Runs as a separate container; matric connects via GLINER_BASE_URL=http://gliner:8090
  # Set GLINER_BASE_URL= (empty) in .env to disable NER entirely.
  gliner:
    image: ${FORTEMI_REGISTRY:-git.integrolabs.net}/fortemi/fortemi:${FORTEMI_GLINER_TAG:-gliner-main}
    build:
      context: build/gliner
      dockerfile: Dockerfile
    volumes:
      - gliner-models:/root/.cache/huggingface/hub
    environment:
      - GLINER_MODEL=${GLINER_MODEL:-urchade/gliner_large-v2.1}
      - GLINER_PORT=8090
      - GLINER_THRESHOLD=${GLINER_THRESHOLD:-0.3}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://0.0.0.0:8090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  matric:
    image: ${FORTEMI_REGISTRY:-git.integrolabs.net}/fortemi/fortemi:${FORTEMI_TAG:-bundle-main}
    build:
      context: .
      dockerfile: Dockerfile.bundle
      args:
        VERSION: ${VERSION:-dev}
        GIT_SHA: ${GIT_SHA:-unknown}
        BUILD_DATE: ${BUILD_DATE:-unknown}
        CARGO_BUILD_JOBS: ${CARGO_BUILD_JOBS:-8}
    ports:
      - "3000:3000"
      - "3001:3001"
    depends_on:
      redis:
        condition: service_healthy
      whisper:
        condition: service_healthy
        required: false  # GPU whisper is optional - matric starts without it
      gliner:
        condition: service_healthy
        required: false  # GLiNER is optional - matric starts without it (NER disabled)
    environment:
      # ── PostgreSQL ──────────────────────────────────────────────────────
      - POSTGRES_USER=matric
      - POSTGRES_PASSWORD=matric
      - POSTGRES_DB=matric

      # ── API Server ─────────────────────────────────────────────────────
      - RUST_LOG=${RUST_LOG:-info}

      # ── Debug ──────────────────────────────────────────────────────────
      - DEBUG_SESSION_CONTEXT=${DEBUG_SESSION_CONTEXT:-}
      - RATE_LIMIT_ENABLED=${RATE_LIMIT_ENABLED:-false}
      - REQUIRE_AUTH=${REQUIRE_AUTH:-false}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-}
      - MATRIC_MAX_BODY_SIZE_BYTES=${MATRIC_MAX_BODY_SIZE_BYTES:-2147483648}

      # ── Redis Cache ────────────────────────────────────────────────────
      - REDIS_ENABLED=${REDIS_ENABLED:-true}
      - REDIS_URL=redis://redis:6379
      - REDIS_CACHE_TTL=${REDIS_CACHE_TTL:-300}

      # ── Full-Text Search ───────────────────────────────────────────────
      - FTS_SCRIPT_DETECTION=${FTS_SCRIPT_DETECTION:-true}
      - FTS_TRIGRAM_FALLBACK=${FTS_TRIGRAM_FALLBACK:-true}
      - FTS_BIGRAM_CJK=${FTS_BIGRAM_CJK:-true}
      - FTS_MULTILINGUAL_CONFIGS=${FTS_MULTILINGUAL_CONFIGS:-true}
      - FTS_WEBSEARCH_TO_TSQUERY=${FTS_WEBSEARCH_TO_TSQUERY:-true}

      # ── File Storage ───────────────────────────────────────────────────
      - FILE_STORAGE_PATH=${FILE_STORAGE_PATH:-/var/lib/matric/files}

      # ── Real-Time Events ───────────────────────────────────────────────
      - MATRIC_EVENT_BUS_CAPACITY=${MATRIC_EVENT_BUS_CAPACITY:-256}
      - MATRIC_WEBHOOK_TIMEOUT_SECS=${MATRIC_WEBHOOK_TIMEOUT_SECS:-10}

      # ── Background Worker ──────────────────────────────────────────────
      # - WORKER_ENABLED=true
      # - LOG_FORMAT=json

      # ── Support Memory Archive ──────────────────────────────────────────
      - DISABLE_SUPPORT_MEMORY=${DISABLE_SUPPORT_MEMORY:-false}
      - MAX_MEMORIES=${MAX_MEMORIES:-10}

      # ── Backup ─────────────────────────────────────────────────────────
      - BACKUP_DEST=${BACKUP_DEST:-/var/backups/matric-memory}

      # ── OAuth ──────────────────────────────────────────────────────────
      # ISSUER_URL is REQUIRED for OAuth/MCP - set in .env
      - ISSUER_URL=${ISSUER_URL:-https://localhost:3000}
      # OAuth token lifetimes (seconds)
      - OAUTH_TOKEN_LIFETIME_SECS=${OAUTH_TOKEN_LIFETIME_SECS:-3600}
      - OAUTH_MCP_TOKEN_LIFETIME_SECS=${OAUTH_MCP_TOKEN_LIFETIME_SECS:-14400}

      # ── MCP Server ─────────────────────────────────────────────────────
      - MCP_TRANSPORT=http
      - MCP_PORT=3001
      - MCP_BASE_URL=${MCP_BASE_URL:-${ISSUER_URL:-https://localhost:3000}/mcp}
      # Internal API URL for MCP→API calls (avoids nginx hairpin)
      - FORTEMI_URL=${FORTEMI_URL:-http://localhost:3000}
      # MCP OAuth client credentials - register via POST /oauth/register
      - MCP_CLIENT_ID=${MCP_CLIENT_ID}
      - MCP_CLIENT_SECRET=${MCP_CLIENT_SECRET}

      # ── Job Worker ─────────────────────────────────────────────────────
      # Max concurrent jobs per worker (Phase 1 jobs run in parallel).
      # Set OLLAMA_NUM_PARALLEL on your Ollama server to match.
      - JOB_MAX_CONCURRENT=${JOB_MAX_CONCURRENT:-4}
      # Fast model for extraction pipeline (concept tagging, references, titles).
      # Default: qwen3:8b (144 tok/s, 40K ctx, strong JSON). Set to empty to disable.
      # Chunk size adapts automatically to model context window via profiles.
      # For 3B models (e.g., granite4:3b): smaller chunks, more passes.
      # For 8B+ models: most notes process in a single pass (no chunking).
      - MATRIC_FAST_GEN_MODEL=${MATRIC_FAST_GEN_MODEL:-qwen3:8b}
      # Timeout for fast model requests (seconds).
      - MATRIC_FAST_GEN_TIMEOUT_SECS=${MATRIC_FAST_GEN_TIMEOUT_SECS:-60}

      # ── Inference: Ollama (default) ────────────────────────────────────
      - MATRIC_INFERENCE_DEFAULT=${MATRIC_INFERENCE_DEFAULT:-ollama}
      - OLLAMA_BASE=${OLLAMA_BASE:-http://host.docker.internal:11434}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://host.docker.internal:11434}
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL:-nomic-embed-text}
      - OLLAMA_GEN_MODEL=${OLLAMA_GEN_MODEL:-gpt-oss:20b}
      - OLLAMA_EMBED_DIM=${OLLAMA_EMBED_DIM:-768}
      - MATRIC_EMBED_TIMEOUT_SECS=${MATRIC_EMBED_TIMEOUT_SECS:-30}
      - MATRIC_GEN_TIMEOUT_SECS=${MATRIC_GEN_TIMEOUT_SECS:-120}

      # ── 3D Model Renderer (Three.js) ───────────────────────────────────
      # Internal renderer URL — bundled Three.js renderer on localhost:8080
      - RENDERER_URL=${RENDERER_URL:-http://localhost:8080}
      - RENDERER_PORT=${RENDERER_PORT:-8080}

      # ── Extraction Services (enabled by default) ─────────────────────
      - OLLAMA_VISION_MODEL=${OLLAMA_VISION_MODEL:-qwen3-vl:8b}
      # Whisper is bundled and enabled by default (GPU). Override for remote server.
      - WHISPER_BASE_URL=${WHISPER_BASE_URL:-http://whisper:8000}
      - WHISPER_MODEL=${WHISPER_MODEL:-Systran/faster-distil-whisper-large-v3}
      - OCR_ENABLED=${OCR_ENABLED:-false}
      - LIBREOFFICE_PATH=${LIBREOFFICE_PATH:-}
      # GLiNER NER (sidecar container, set to empty to disable)
      - GLINER_BASE_URL=${GLINER_BASE_URL:-http://gliner:8090}
      # Target concepts per note. GLiNER runs first; if it produces fewer than
      # this, fast/standard LLM supplements. Higher = richer taxonomy, slower.
      - EXTRACTION_TARGET_CONCEPTS=${EXTRACTION_TARGET_CONCEPTS:-5}

      # ── Inference: OpenAI (alternative) ────────────────────────────────
      # - OPENAI_API_KEY=${OPENAI_API_KEY}
      # - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
      # - OPENAI_EMBED_MODEL=${OPENAI_EMBED_MODEL:-text-embedding-3-small}
      # - OPENAI_GEN_MODEL=${OPENAI_GEN_MODEL:-gpt-4o-mini}
      # - OPENAI_EMBED_DIM=${OPENAI_EMBED_DIM:-1536}
      # - OPENAI_TIMEOUT=${OPENAI_TIMEOUT:-30}
    volumes:
      # PostgreSQL data persistence
      - matric-pgdata:/var/lib/postgresql/data
      # File storage (attachments, blobs)
      - matric-files:/var/lib/matric/files
      # Backup directory
      - matric-backups:/var/backups/matric-memory
    restart: unless-stopped
    extra_hosts:
      # Required on Linux for host.docker.internal to resolve to host
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 5s
      start_period: 60s
      retries: 3

volumes:
  matric-pgdata:
    driver: local
  matric-files:
    driver: local
  matric-backups:
    driver: local
  matric-redis:
    driver: local
  whisper-models:
    driver: local
  gliner-models:
    driver: local
